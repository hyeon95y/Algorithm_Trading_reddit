{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [BERT to the rescue!](https://towardsdatascience.com/bert-to-the-rescue-17671379687f)\n",
    "- [Understanding BERT Part 2: BERT Specifics\n",
    "](https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "font_dirs = ['/usr/share/fonts/truetype/nanum']\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    "font_list = font_manager.createFontList(font_files)\n",
    "font_manager.fontManager.ttflist.extend(font_list)\n",
    "plt.rcParams['font.family'] = 'NanumGothic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "from torch import nn\n",
    "from torchnlp.datasets import imdb_dataset\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn.seed(321)\n",
    "np.random.seed(321)\n",
    "torch.manual_seed(321)\n",
    "torch.cuda.manual_seed(321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = imdb_dataset(train=True, test=True)\n",
    "rn.shuffle(train_data)\n",
    "rn.shuffle(test_data)\n",
    "train_data = train_data[:1000]\n",
    "test_data = test_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'This review contains spoilers for those who are not aware of the details of the true story on which this movie is based.<br /><br />The right to be presumed \"Innocent until proven guilty\" is a basic entitlement of anyone in a civilised society; but according to Fred Schepisi\\'s partisan but sadly convincing story of a famous Australian murder trial, it was not granted to Lindy Chamberlain, accused of killing her baby. The story suggesting her innocence was unlikely (a dingo was alleged to have taken it), but those implying her guilt even more so, and there was no solid evidence against her. But the Australian public was transfixed by the possibility of her guilt, and the deeply religious Chamberlains appeared creepy when appearing in the media (and the media themselves, of course, were anything but innocent in this process). So although cleared by an initial inquest, they were later prosecuted and convicted. Although Chamberlain was eventually released, this shamefully only followed the discovery of new evidence \"proving\" their innocence, something no defendants should have to produce.<br /><br />\\'A Cry in the Dark\\' is well acted throughout, especially by Meryl Streep, who puts on a convincing Australian accent (at least to this Pom\\'s ears) and manages keep Lindy sympathetic (to us) while still conveying how she managed to become a national hate figure. The scenes where she actually gets imprisoned are simple but heartbreaking, because we believe in the characters as real. <br /><br />Regardless of the accuracy of its portrayal of this story (something I can\\'t comment on), the wider theme of this film will ring horribly true to anyone with a passing knowledge of the British popular press and its ruthless habit of appealing directly to their readership\\'s least charitable instincts. No legal system will ever be perfect; but the current cry against asylum seekers in contemporary British tabloids comes from exactly the same pit of evil as the voices that put Lindy Chamberlain away. I\\'m not a religious man, but the Bible still contains some killer lines (if you\\'ll excuse the pun). \"Judge not lest ye be judged\" is one of them.',\n",
       " 'sentiment': 'pos'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000, 100, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), train_data)))\n",
    "test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), test_data)))\n",
    "\n",
    "len(train_texts), len(train_labels), len(test_texts), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This review contains spoilers for those who are not aware of the details of the true story on which this movie is based.<br /><br />The right to be presumed \"Innocent until proven guilty\" is a basic entitlement of anyone in a civilised society; but according to Fred Schepisi\\'s partisan but sadly convincing story of a famous Australian murder trial, it was not granted to Lindy Chamberlain, accused of killing her baby. The story suggesting her innocence was unlikely (a dingo was alleged to have taken it), but those implying her guilt even more so, and there was no solid evidence against her. But the Australian public was transfixed by the possibility of her guilt, and the deeply religious Chamberlains appeared creepy when appearing in the media (and the media themselves, of course, were anything but innocent in this process). So although cleared by an initial inquest, they were later prosecuted and convicted. Although Chamberlain was eventually released, this shamefully only followed the discovery of new evidence \"proving\" their innocence, something no defendants should have to produce.<br /><br />\\'A Cry in the Dark\\' is well acted throughout, especially by Meryl Streep, who puts on a convincing Australian accent (at least to this Pom\\'s ears) and manages keep Lindy sympathetic (to us) while still conveying how she managed to become a national hate figure. The scenes where she actually gets imprisoned are simple but heartbreaking, because we believe in the characters as real. <br /><br />Regardless of the accuracy of its portrayal of this story (something I can\\'t comment on), the wider theme of this film will ring horribly true to anyone with a passing knowledge of the British popular press and its ruthless habit of appealing directly to their readership\\'s least charitable instincts. No legal system will ever be perfect; but the current cry against asylum seekers in contemporary British tabloids comes from exactly the same pit of evil as the voices that put Lindy Chamberlain away. I\\'m not a religious man, but the Bible still contains some killer lines (if you\\'ll excuse the pun). \"Judge not lest ye be judged\" is one of them.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'my', 'name', 'is', 'dim', '##a']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('Hi my name is Dima')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'review', 'contains', 'spoil', '##ers', 'for', 'those', 'who', 'are', 'not', 'aware', 'of', 'the', 'details', 'of', 'the', 'true', 'story', 'on', 'which', 'this', 'movie', 'is', 'based', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'right', 'to', 'be', 'presumed', '\"', 'innocent', 'until', 'proven', 'guilty', '\"', 'is', 'a', 'basic', 'en', '##ti', '##tlement', 'of', 'anyone', 'in', 'a', 'civil', '##ised', 'society', ';', 'but', 'according', 'to', 'fred', 'sc', '##he', '##pis', '##i', \"'\", 's', 'partisan', 'but', 'sadly', 'convincing', 'story', 'of', 'a', 'famous', 'australian', 'murder', 'trial', ',', 'it', 'was', 'not', 'granted', 'to', 'lin', '##dy', 'chamberlain', ',', 'accused', 'of', 'killing', 'her', 'baby', '.', 'the', 'story', 'suggesting', 'her', 'innocence', 'was', 'unlikely', '(', 'a', 'ding', '##o', 'was', 'alleged', 'to', 'have', 'taken', 'it', ')', ',', 'but', 'those', 'implying', 'her', 'guilt', 'even', 'more', 'so', ',', 'and', 'there', 'was', 'no', 'solid', 'evidence', 'against', 'her', '.', 'but', 'the', 'australian', 'public', 'was', 'trans', '##fixed', 'by', 'the', 'possibility', 'of', 'her', 'guilt', ',', 'and', 'the', 'deeply', 'religious', 'chamberlain', '##s', 'appeared', 'creepy', 'when', 'appearing', 'in', 'the', 'media', '(', 'and', 'the', 'media', 'themselves', ',', 'of', 'course', ',', 'were', 'anything', 'but', 'innocent', 'in', 'this', 'process', ')', '.', 'so', 'although', 'cleared', 'by', 'an', 'initial', 'in', '##quest', ',', 'they', 'were', 'later', 'prosecuted', 'and', 'convicted', '.', 'although', 'chamberlain', 'was', 'eventually', 'released', ',', 'this', 'shame', '##fully', 'only', 'followed', 'the', 'discovery', 'of', 'new', 'evidence', '\"', 'proving', '\"', 'their', 'innocence', ',', 'something', 'no', 'defendants', 'should', 'have', 'to', 'produce', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', \"'\", 'a', 'cry', 'in', 'the', 'dark', \"'\", 'is', 'well', 'acted', 'throughout', ',', 'especially', 'by', 'mer', '##yl', 'st', '##ree', '##p', ',', 'who', 'puts', 'on', 'a', 'convincing', 'australian', 'accent', '(', 'at', 'least', 'to', 'this', 'po', '##m', \"'\", 's', 'ears', ')', 'and', 'manages', 'keep', 'lin', '##dy', 'sympathetic', '(', 'to', 'us', ')', 'while', 'still', 'convey', '##ing', 'how', 'she', 'managed', 'to', 'become', 'a', 'national', 'hate', 'figure', '.', 'the', 'scenes', 'where', 'she', 'actually', 'gets', 'imprisoned', 'are', 'simple', 'but', 'heartbreak', '##ing', ',', 'because', 'we', 'believe', 'in', 'the', 'characters', 'as', 'real', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'regardless', 'of', 'the', 'accuracy', 'of', 'its', 'portrayal', 'of', 'this', 'story', '(', 'something', 'i', 'can', \"'\", 't', 'comment', 'on', ')', ',', 'the', 'wider', 'theme', 'of', 'this', 'film', 'will', 'ring', 'horribly', 'true', 'to', 'anyone', 'with', 'a', 'passing', 'knowledge', 'of', 'the', 'british', 'popular', 'press', 'and', 'its', 'ruthless', 'habit', 'of', 'appealing', 'directly', 'to', 'their', 'readers', '##hip', \"'\", 's', 'least', 'charitable', 'instincts', '.', 'no', 'legal', 'system', 'will', 'ever', 'be', 'perfect', ';', 'but', 'the', 'current', 'cry', 'against', 'asylum', 'seekers', 'in', 'contemporary', 'british', 'tabloid', '##s', 'comes', 'from', 'exactly', 'the', 'same', 'pit', 'of', 'evil', 'as', 'the', 'voices', 'that', 'put', 'lin', '##dy', 'chamberlain', 'away', '.', 'i', \"'\", 'm', 'not', 'a', 'religious', 'man', ',', 'but', 'the', 'bible', 'still', 'contains', 'some', 'killer', 'lines', '(', 'if', 'you', \"'\", 'll', 'excuse', 'the', 'pun', ')', '.', '\"', 'judge', 'not', 'lest', 'ye', 'be', 'judged', '\"', 'is', 'one', 'of', 'them', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train_texts))\n",
    "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], test_texts))\n",
    "\n",
    "len(train_tokens), len(test_tokens)\n",
    "print(train_tokens[0])\n",
    "#print(test_tokens[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 512), (100, 512))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "test_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "\n",
    "train_tokens_ids.shape, test_tokens_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  2023,  3319,  3397, 27594,  2545,  2005,  2216,  2040,\n",
       "        2024,  2025,  5204,  1997,  1996,  4751,  1997,  1996,  2995,\n",
       "        2466,  2006,  2029,  2023,  3185,  2003,  2241,  1012,  1026,\n",
       "        7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,  2157,\n",
       "        2000,  2022, 14609,  1000,  7036,  2127, 10003,  5905,  1000,\n",
       "        2003,  1037,  3937,  4372,  3775, 24007,  1997,  3087,  1999,\n",
       "        1037,  2942,  5084,  2554,  1025,  2021,  2429,  2000,  5965,\n",
       "        8040,  5369, 18136,  2072,  1005,  1055, 14254,  2021, 13718,\n",
       "       13359,  2466,  1997,  1037,  3297,  2827,  4028,  3979,  1010,\n",
       "        2009,  2001,  2025,  4379,  2000, 11409,  5149, 13904,  1010,\n",
       "        5496,  1997,  4288,  2014,  3336,  1012,  1996,  2466,  9104,\n",
       "        2014, 12660,  2001,  9832,  1006,  1037, 22033,  2080,  2001,\n",
       "        6884,  2000,  2031,  2579,  2009,  1007,  1010,  2021,  2216,\n",
       "       20242,  2014,  8056,  2130,  2062,  2061,  1010,  1998,  2045,\n",
       "        2001,  2053,  5024,  3350,  2114,  2014,  1012,  2021,  1996,\n",
       "        2827,  2270,  2001,  9099, 23901,  2011,  1996,  6061,  1997,\n",
       "        2014,  8056,  1010,  1998,  1996,  6171,  3412, 13904,  2015,\n",
       "        2596, 17109,  2043,  6037,  1999,  1996,  2865,  1006,  1998,\n",
       "        1996,  2865,  3209,  1010,  1997,  2607,  1010,  2020,  2505,\n",
       "        2021,  7036,  1999,  2023,  2832,  1007,  1012,  2061,  2348,\n",
       "        5985,  2011,  2019,  3988,  1999, 15500,  1010,  2027,  2020,\n",
       "        2101, 21651,  1998,  7979,  1012,  2348, 13904,  2001,  2776,\n",
       "        2207,  1010,  2023,  9467,  7699,  2069,  2628,  1996,  5456,\n",
       "        1997,  2047,  3350,  1000, 13946,  1000,  2037, 12660,  1010,\n",
       "        2242,  2053, 16362,  2323,  2031,  2000,  3965,  1012,  1026,\n",
       "        7987,  1013,  1028,  1026,  7987,  1013,  1028,  1005,  1037,\n",
       "        5390,  1999,  1996,  2601,  1005,  2003,  2092,  6051,  2802,\n",
       "        1010,  2926,  2011, 21442,  8516,  2358,  9910,  2361,  1010,\n",
       "        2040,  8509,  2006,  1037, 13359,  2827,  9669,  1006,  2012,\n",
       "        2560,  2000,  2023, 13433,  2213,  1005,  1055,  5551,  1007,\n",
       "        1998,  9020,  2562, 11409,  5149, 13026,  1006,  2000,  2149,\n",
       "        1007,  2096,  2145, 16636,  2075,  2129,  2016,  3266,  2000,\n",
       "        2468,  1037,  2120,  5223,  3275,  1012,  1996,  5019,  2073,\n",
       "        2016,  2941,  4152,  8580,  2024,  3722,  2021, 27724,  2075,\n",
       "        1010,  2138,  2057,  2903,  1999,  1996,  3494,  2004,  2613,\n",
       "        1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,\n",
       "        7539,  1997,  1996, 10640,  1997,  2049, 13954,  1997,  2023,\n",
       "        2466,  1006,  2242,  1045,  2064,  1005,  1056,  7615,  2006,\n",
       "        1007,  1010,  1996,  7289,  4323,  1997,  2023,  2143,  2097,\n",
       "        3614, 27762,  2995,  2000,  3087,  2007,  1037,  4458,  3716,\n",
       "        1997,  1996,  2329,  2759,  2811,  1998,  2049, 18101, 10427,\n",
       "        1997, 16004,  3495,  2000,  2037,  8141,  5605,  1005,  1055,\n",
       "        2560, 11128, 16160,  1012,  2053,  3423,  2291,  2097,  2412,\n",
       "        2022,  3819,  1025,  2021,  1996,  2783,  5390,  2114, 11386,\n",
       "       24071,  1999,  3824,  2329, 24173,  2015,  3310,  2013,  3599,\n",
       "        1996,  2168,  6770,  1997,  4763,  2004,  1996,  5755,  2008,\n",
       "        2404, 11409,  5149, 13904,  2185,  1012,  1045,  1005,  1049,\n",
       "        2025,  1037,  3412,  2158,  1010,  2021,  1996,  6331,  2145,\n",
       "        3397,  2070,  6359,  3210,  1006,  2065,  2017,  1005,  2222,\n",
       "        8016,  1996, 26136,  1007,  1012,  1000,  3648,  2025, 26693,\n",
       "        6300,  2022, 13224,  1000,  2003,  2028,  1997,  2068,  1012,\n",
       "         102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000,), (100,), 0.489, 0.5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = np.array(train_labels) == 'pos'\n",
    "test_y = np.array(test_labels) == 'pos'\n",
    "train_y.shape, test_y.shape, np.mean(train_y), np.mean(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
    "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(train_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "baseline_model = make_pipeline(CountVectorizer(ngram_range=(1,3)), LogisticRegression()).fit(train_texts, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_predicted = baseline_model.predict(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.78      0.78      0.78        50\n",
      "         pos       0.78      0.78      0.78        50\n",
      "\n",
      "    accuracy                           0.78       100\n",
      "   macro avg       0.78      0.78      0.78       100\n",
      "weighted avg       0.78      0.78      0.78       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, baseline_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertBinaryClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens, masks=None):\n",
    "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_clf = BertBinaryClassifier()\n",
    "bert_clf = bert_clf.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 512]), torch.Size([3, 512, 768]), torch.Size([3, 768]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(train_tokens_ids[:3]).to(device)\n",
    "y, pooled = bert_clf.bert(x, output_all_encoded_layers=False)\n",
    "x.shape, y.shape, pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.495686  ],\n",
       "       [0.41647682],\n",
       "       [0.4135548 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = bert_clf(x)\n",
    "y.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x, pooled = None, None, None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'439.065088M'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
    "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
    "\n",
    "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
    "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
    "\n",
    "train_masks_tensor = torch.tensor(train_masks)\n",
    "test_masks_tensor = torch.tensor(test_masks)\n",
    "\n",
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(bert_clf.sigmoid.named_parameters()) \n",
    "optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(bert_clf.parameters(), lr=3e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "249/250.0 loss: 0.6401839823722839 \n"
     ]
    }
   ],
   "source": [
    "for epoch_num in range(EPOCHS):\n",
    "    bert_clf.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
    "        print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n",
    "        logits = bert_clf(token_ids, masks)\n",
    "        \n",
    "        loss_func = nn.BCELoss()\n",
    "\n",
    "        batch_loss = loss_func(logits, labels)\n",
    "        train_loss += batch_loss.item()\n",
    "        \n",
    "        \n",
    "        bert_clf.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        \n",
    "\n",
    "        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print('Epoch: ', epoch_num + 1)\n",
    "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_clf.eval()\n",
    "bert_predicted = []\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(test_dataloader):\n",
    "\n",
    "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
    "\n",
    "        logits = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        loss = loss_func(logits, labels)\n",
    "        numpy_logits = logits.cpu().detach().numpy()\n",
    "        \n",
    "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
    "        all_logits += list(numpy_logits[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(bert_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.72      0.77        50\n",
      "        True       0.75      0.84      0.79        50\n",
      "\n",
      "    accuracy                           0.78       100\n",
      "   macro avg       0.78      0.78      0.78       100\n",
      "weighted avg       0.78      0.78      0.78       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, bert_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
