{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [SKTBrain/KoBERT](https://github.com/SKTBrain/KoBERT)\n",
    "- [eagle705/pytorch-bert-crf-ner](https://github.com/eagle705/pytorch-bert-crf-ner/blob/master/Visualization_BERT_NER.ipynb)\n",
    "- [BERT to the rescue!](https://towardsdatascience.com/bert-to-the-rescue-17671379687f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "font_dirs = ['/usr/share/fonts/truetype/nanum']\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    "font_list = font_manager.createFontList(font_files)\n",
    "font_manager.fontManager.ttflist.extend(font_list)\n",
    "plt.rcParams['font.family'] = 'NanumGothic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SKT BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[██████████████████████████████████████████████████]\n",
      "[██████████████████████████████████████████████████]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(8002, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from kobert.utils import get_tokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, vocab  = get_pytorch_kobert_model()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenized input\n",
    "#text = [\"누가 기침소리를 내었는가 ? 누구인가 ?\"]\n",
    "#tokenized_text = list(map(lambda t: sp.EncodeAsPieces(t), text))\n",
    "#print(tokenized_text[0])\n",
    "#indexed_tokens = list(map(vocab.to_indices, tokenized_text))\n",
    "#print(indexed_tokens[0])\n",
    "#reconstructed = list(map(vocab.to_tokens, indexed_tokens))\n",
    "#print(reconstructed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[██████████████████████████████████████████████████]\n",
      "/root/kobert/tokenizer_78b3253a26.model\n",
      "<sentencepiece.SentencePieceProcessor; proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x7fb95c838e40> >\n",
      "누가 기침소리를 내었는가 ? 누구인가 ?\n",
      "['▁누가', '▁기', '침', '소리', '를', '▁내', '었', '는', '가', '▁?', '▁누구', '인', '가', '▁?']\n",
      "[1527, 1258, 7491, 6609, 6116, 1434, 6885, 5760, 5330, 633, 1528, 7119, 5330, 633]\n",
      "['▁누가', '▁기', '침', '소리', '를', '▁내', '었', '는', '가', '▁?', '▁누구', '인', '가', '▁?']\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer\n",
    "tok_path = get_tokenizer()\n",
    "print(tok_path)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "print(sp)\n",
    "sp.Load(tok_path)\n",
    "\n",
    "# Tokenized input\n",
    "#text = \"[CLS] 누가 기침소리를 내었는가 ? [SEP] 누구인가 ? [SEP]\"\n",
    "text = \"누가 기침소리를 내었는가 ? 누구인가 ?\"\n",
    "tokenized_text = sp.EncodeAsPieces(text)\n",
    "indexed_tokens = vocab.to_indices(tokenized_text)\n",
    "reconstructed = vocab.to_tokens(indexed_tokens)\n",
    "\n",
    "print(text)\n",
    "print(tokenized_text)\n",
    "print(indexed_tokens)\n",
    "print(reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5272, 248, 1175, 4526, 12, 121, 1528, 10, 11, 4859, 2861, 24, 11, 4859]\n"
     ]
    }
   ],
   "source": [
    "indexed_tokens = sp.EncodeAsIds(text)\n",
    "print(indexed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2461,  0.2428,  0.2590,  ..., -0.4861, -0.0731,  0.0756],\n",
      "        [-0.2478,  0.2420,  0.2552,  ..., -0.4877, -0.0727,  0.0754],\n",
      "        [-0.2472,  0.2420,  0.2561,  ..., -0.4874, -0.0733,  0.0765]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 768])\n",
      "tensor([[-9.0314e-02, -4.4431e-02,  1.5792e-01,  3.1426e-02, -7.4218e-01,\n",
      "          3.7952e-01,  8.3690e-02, -8.2061e-02,  5.9057e-02, -7.1027e-02,\n",
      "         -3.6518e-01, -1.0447e-01, -9.4905e-02,  1.4371e-01, -1.0304e-02,\n",
      "          4.4356e-01, -5.9710e-01,  2.8139e-03,  8.4893e-02,  8.0748e-02,\n",
      "         -1.3730e-01, -2.8074e-02,  8.6585e-02, -1.0943e-01,  1.1957e-04,\n",
      "          4.2622e-01, -7.0176e-01, -1.3383e-01, -8.7509e-01, -9.9335e-02,\n",
      "          8.5697e-01,  5.5653e-01, -1.6640e-02, -1.1087e-01,  2.0848e-01,\n",
      "          5.8470e-01, -1.1724e-02, -7.3793e-02, -4.2836e-01, -6.7545e-02,\n",
      "         -7.8296e-02, -6.5179e-02, -5.5545e-02,  6.5026e-01, -9.8818e-01,\n",
      "          6.5573e-02, -1.5565e-01, -4.3171e-02, -9.8807e-01,  9.4174e-01,\n",
      "          2.3175e-01, -9.9838e-01, -9.5131e-01,  3.1647e-02, -2.5830e-03,\n",
      "         -6.7435e-01,  1.0332e-01, -6.3440e-01,  6.6300e-02, -2.8817e-02,\n",
      "          5.3691e-03, -1.8036e-01,  8.6036e-01, -7.1559e-01, -5.2901e-02,\n",
      "          7.6154e-02,  9.3047e-02, -3.7840e-03,  1.0834e-01,  1.6944e-02,\n",
      "          8.8734e-01, -1.9038e-02, -2.2431e-01, -4.7912e-02, -4.6557e-01,\n",
      "          7.6835e-02, -3.1749e-01,  7.6349e-02,  3.3167e-01,  1.1159e-01,\n",
      "         -2.8926e-02, -4.1216e-01, -3.8193e-01,  4.4262e-01, -9.3865e-02,\n",
      "         -1.1985e-02, -1.0765e-01, -1.4020e-02,  3.2103e-01,  7.8825e-01,\n",
      "         -1.9590e-01,  3.5568e-02, -4.7788e-02, -9.3513e-02, -3.3240e-01,\n",
      "         -3.6088e-03,  8.0596e-02,  1.9113e-02,  9.5772e-02, -5.8218e-01,\n",
      "          7.1050e-02,  1.1238e-02,  9.9498e-02, -5.0844e-02, -6.1773e-01,\n",
      "          1.2965e-02, -1.3467e-01, -8.2426e-02,  1.5885e-01,  1.7332e-01,\n",
      "         -6.1233e-02,  8.2601e-02, -2.5352e-01,  2.0043e-02, -4.9155e-01,\n",
      "          4.8146e-02,  2.2087e-03,  1.7180e-02,  9.2277e-02, -3.3122e-02,\n",
      "          2.3940e-02, -5.7843e-02, -6.6485e-01,  8.3922e-01, -9.9894e-01,\n",
      "         -3.8994e-02, -1.1066e-01, -6.7163e-03,  6.5162e-02,  9.8453e-03,\n",
      "         -5.7394e-02, -2.6451e-02, -3.8337e-02, -4.6768e-02,  1.2441e-02,\n",
      "          1.6111e-01,  6.2509e-01, -6.9837e-02,  1.0481e-01,  5.3441e-02,\n",
      "         -6.1491e-02,  1.5976e-02,  3.3276e-01, -7.3973e-01, -5.3669e-02,\n",
      "         -1.5295e-02, -4.2157e-02,  5.5983e-02, -9.0542e-02,  4.1725e-02,\n",
      "         -7.5838e-01, -4.1056e-02, -3.1149e-01,  4.5713e-02,  1.4176e-02,\n",
      "         -5.4661e-02, -6.8062e-02,  2.6319e-02, -8.6462e-02, -7.5507e-01,\n",
      "          4.3413e-02, -1.3654e-02, -2.3135e-02,  9.1704e-01,  3.2953e-02,\n",
      "          3.3777e-02, -6.5220e-02, -6.5350e-02,  8.0596e-02,  3.2013e-02,\n",
      "          1.2159e-01,  3.7620e-02, -8.5716e-01,  5.1438e-01,  6.4800e-03,\n",
      "          9.9809e-01, -8.4303e-02, -2.4880e-01, -9.2737e-03, -2.3128e-02,\n",
      "         -7.5584e-02,  7.5837e-01,  2.2938e-01, -9.9882e-01, -1.7415e-02,\n",
      "         -1.3812e-02,  1.0398e-01,  6.7775e-02, -5.0627e-02, -4.3661e-01,\n",
      "         -1.1267e-01, -5.1147e-04, -5.3875e-01, -5.7024e-02, -5.0544e-02,\n",
      "          1.2913e-02,  2.0323e-01, -7.2005e-02, -1.6688e-01,  1.9091e-02,\n",
      "          5.1721e-03,  6.1205e-01,  3.7344e-02, -3.1527e-02,  8.7065e-01,\n",
      "         -5.9752e-02, -6.4937e-02,  2.6950e-02, -1.5833e-01,  3.1401e-01,\n",
      "          1.2504e-02,  1.2054e-01,  8.6126e-01,  8.2400e-01, -6.1186e-02,\n",
      "          2.9007e-03, -3.5148e-02,  2.6882e-01,  8.2610e-01, -1.1824e-01,\n",
      "         -3.6852e-01, -7.9212e-02,  6.4023e-02, -2.7358e-02,  1.0596e-01,\n",
      "         -1.5724e-01,  7.6896e-01, -7.7541e-02, -8.4581e-02,  7.1178e-02,\n",
      "         -9.2642e-01,  1.6214e-01,  1.3090e-01,  7.3327e-02,  2.4008e-01,\n",
      "         -3.4433e-01,  8.1896e-01,  2.2810e-03,  9.2115e-03, -2.6237e-02,\n",
      "          3.9763e-01, -6.6031e-02,  8.3374e-01, -1.6444e-02,  8.5033e-01,\n",
      "         -2.3064e-03,  2.0880e-02,  6.6120e-01,  1.1458e-01,  6.3873e-01,\n",
      "         -2.4924e-02, -1.9475e-02,  5.0935e-02,  3.8568e-02, -5.1232e-02,\n",
      "         -1.2023e-01, -2.1858e-01,  1.0702e-01,  7.5702e-01, -4.6442e-01,\n",
      "         -5.5578e-02, -6.2425e-02, -1.5191e-02,  9.0927e-02,  2.9525e-01,\n",
      "          6.1124e-02,  1.4597e-03, -7.1453e-01, -5.0086e-01,  6.0738e-02,\n",
      "          3.1266e-02, -4.1778e-02,  4.1208e-02,  6.2514e-01,  5.9181e-01,\n",
      "         -8.5658e-01,  4.1398e-03, -4.2715e-02, -8.0147e-02, -5.6312e-02,\n",
      "          2.0564e-02,  7.8113e-01, -7.7442e-01, -2.1610e-02,  3.6128e-02,\n",
      "         -1.1723e-02,  2.2410e-03, -1.5298e-05, -2.0352e-02, -1.2131e-01,\n",
      "          6.6504e-01,  2.6037e-02, -3.9642e-01,  5.9964e-02, -3.8031e-02,\n",
      "          2.3147e-02, -3.5461e-01,  7.7344e-01,  1.9509e-01,  2.0505e-02,\n",
      "          3.1563e-03,  4.6691e-01,  3.2952e-02, -7.6956e-02,  9.9792e-01,\n",
      "          1.1237e-02, -7.5386e-01, -7.2844e-01, -2.8443e-02,  5.2630e-02,\n",
      "          3.5432e-01,  6.3295e-02,  5.6675e-03,  5.6776e-01,  9.9925e-01,\n",
      "          7.4896e-02,  7.5899e-02, -2.5422e-02, -1.0333e-01, -1.6573e-01,\n",
      "          1.0926e-02, -2.2113e-02,  9.6460e-02,  2.4344e-02,  3.6222e-03,\n",
      "         -5.7158e-04,  1.1271e-01,  3.2878e-02, -1.2446e-01,  7.5874e-02,\n",
      "         -4.0004e-02, -2.6355e-02, -8.4984e-01,  6.1528e-01, -4.0935e-01,\n",
      "         -4.6947e-02, -5.2872e-02,  4.0501e-02, -6.8283e-02,  8.7240e-01,\n",
      "         -9.4471e-03, -8.1216e-02,  6.9631e-01, -1.5412e-01,  4.5344e-01,\n",
      "         -9.9441e-01, -5.2850e-02, -2.6124e-01, -4.2744e-02,  2.3114e-02,\n",
      "          8.6734e-03, -7.5113e-02, -8.9416e-02, -1.6106e-02, -5.1416e-02,\n",
      "          9.2850e-02, -5.7907e-01,  9.9862e-01,  5.5906e-03, -8.8403e-02,\n",
      "         -8.1817e-02, -3.5519e-02, -7.0934e-01, -1.8339e-01, -6.8476e-02,\n",
      "          4.4195e-03, -5.7513e-01, -3.4569e-02,  6.4647e-02,  5.5330e-03,\n",
      "         -3.0698e-01,  6.5975e-01, -7.8525e-02,  4.3818e-02,  1.4380e-02,\n",
      "          2.8593e-02, -4.5615e-01,  6.5649e-01, -8.7102e-02, -2.3448e-02,\n",
      "         -9.9830e-01,  5.4821e-03,  8.9399e-01,  2.7689e-01,  5.2683e-02,\n",
      "          7.6412e-02,  2.5127e-02,  1.2044e-02,  1.1589e-01,  4.5321e-02,\n",
      "         -1.1064e-01,  9.9772e-01, -4.4698e-01, -3.8849e-01,  1.0427e-02,\n",
      "         -5.3377e-01, -7.1446e-02,  6.2411e-02,  5.8820e-02, -5.0897e-02,\n",
      "          2.0484e-02,  7.9605e-02,  1.5536e-01, -3.5991e-01,  1.9688e-01,\n",
      "          8.9524e-01, -7.6532e-01,  2.1149e-01, -3.7641e-01,  1.4326e-02,\n",
      "          3.7660e-02, -9.6332e-01, -3.6127e-02,  3.6510e-02, -1.0288e-01,\n",
      "         -5.7785e-01,  3.4572e-02, -2.6450e-02,  4.8152e-02,  2.4057e-02,\n",
      "          6.2639e-02,  7.0044e-01,  1.3003e-02,  4.3146e-01,  1.5576e-01,\n",
      "          2.4087e-02, -2.0372e-02, -6.1189e-01,  9.6252e-02,  1.5901e-01,\n",
      "          1.3572e-01,  4.4082e-02, -6.3417e-01, -6.8879e-01, -2.6707e-01,\n",
      "          1.1769e-01,  2.9819e-01, -4.9979e-01, -4.6869e-01,  3.0008e-01,\n",
      "          7.4650e-01,  6.0712e-03,  9.7434e-02,  9.9899e-01, -4.4430e-01,\n",
      "          1.0142e-01, -5.1609e-02,  3.0279e-01,  6.1115e-01,  3.0389e-02,\n",
      "         -6.4598e-01,  8.4924e-02,  5.6808e-01, -3.6678e-02,  6.4545e-02,\n",
      "          4.0331e-01,  2.6545e-02, -5.9034e-01, -1.0648e-01,  4.4836e-02,\n",
      "         -6.3619e-03, -7.9639e-02, -1.9208e-02,  5.0766e-03, -4.7481e-02,\n",
      "          8.2171e-01,  1.1884e-01, -8.5065e-01,  1.1289e-02, -4.9215e-01,\n",
      "         -1.6174e-01, -8.3348e-02, -3.2785e-02, -1.6595e-01, -1.0728e-02,\n",
      "         -6.7698e-02, -1.2142e-01, -3.7773e-03,  1.1084e-01,  1.8421e-02,\n",
      "         -9.9828e-01, -6.9275e-01,  5.1014e-01, -1.8564e-01, -2.0150e-01,\n",
      "         -2.5576e-01, -1.3861e-01,  8.9640e-02,  4.7162e-02, -9.9058e-01,\n",
      "          5.0023e-02,  2.9389e-02, -3.8238e-02, -4.5870e-02, -8.3963e-01,\n",
      "          7.3587e-02,  7.3426e-01, -5.4345e-04,  6.1135e-02, -3.8408e-01,\n",
      "         -8.5322e-01, -6.1425e-02,  4.0795e-02, -2.6134e-01,  8.6320e-02,\n",
      "          5.1381e-01,  1.3489e-01, -6.9785e-01,  9.1572e-01, -1.3339e-02,\n",
      "          3.5180e-02, -5.9987e-01, -1.1428e-01, -6.9480e-01, -4.4655e-02,\n",
      "          4.9879e-01, -4.0356e-02,  1.9094e-02,  5.0713e-02,  4.2878e-02,\n",
      "         -1.7425e-02, -6.3654e-02,  4.3639e-02, -6.9016e-01,  1.8902e-03,\n",
      "         -4.0964e-01, -1.1383e-01, -7.1528e-02,  5.2935e-02, -2.0510e-02,\n",
      "          4.0144e-01, -6.7733e-02, -9.5959e-01,  6.8566e-02,  7.7552e-01,\n",
      "          6.6642e-01, -2.3484e-02, -9.5805e-02, -2.3137e-02, -9.9553e-01,\n",
      "          3.9244e-02, -9.9939e-01,  1.6120e-01,  2.7891e-01, -2.1599e-03,\n",
      "          1.8922e-03, -1.4927e-01,  2.4562e-02, -5.4461e-01, -4.3355e-01,\n",
      "          2.2279e-02, -9.9913e-01, -8.5562e-01, -2.9425e-01,  5.5037e-02,\n",
      "          7.3747e-01, -4.3466e-02,  5.6507e-01, -4.0932e-02, -3.1574e-02,\n",
      "         -1.5121e-01,  6.7271e-02,  3.2025e-02,  3.1126e-03, -5.3729e-01,\n",
      "          3.0885e-02, -1.2666e-01, -7.6214e-02, -1.8861e-02, -3.5066e-01,\n",
      "         -6.2793e-02,  7.6859e-02, -1.2170e-01, -2.4414e-01, -2.8058e-02,\n",
      "          3.7358e-02, -3.1608e-02,  4.3959e-01, -1.8598e-02, -1.5192e-01,\n",
      "         -1.3698e-02,  1.5353e-01,  7.8235e-02, -3.9719e-02,  3.8397e-01,\n",
      "         -3.8812e-02, -1.4189e-02, -8.7511e-01,  8.4346e-02,  5.9513e-01,\n",
      "         -4.5273e-01,  6.0856e-01,  9.2615e-01, -1.1388e-01,  6.1345e-01,\n",
      "          5.4191e-02, -6.2261e-01, -5.0328e-02,  2.6401e-01,  5.3169e-03,\n",
      "         -3.8857e-02, -4.7759e-02,  9.7082e-02,  7.5794e-02, -9.9631e-01,\n",
      "          1.1467e-01,  3.1150e-01, -3.9708e-01,  1.5324e-01, -2.0588e-02,\n",
      "         -5.5906e-01,  7.4084e-02,  1.0646e-01, -4.2189e-02,  1.5251e-02,\n",
      "         -3.0960e-02, -9.3060e-02, -8.0182e-01,  1.2741e-02,  7.9758e-01,\n",
      "         -5.4955e-02,  1.6908e-02, -3.6949e-02,  6.7666e-02,  9.5799e-02,\n",
      "         -1.7086e-02,  5.2665e-03,  2.7655e-01,  7.8689e-01,  6.2741e-01,\n",
      "          7.4478e-01,  3.9976e-02, -3.6003e-01, -1.6990e-02, -6.0120e-02,\n",
      "          8.6771e-03,  7.3732e-01,  3.6682e-02,  9.9941e-01,  5.9420e-01,\n",
      "         -8.6914e-01,  3.3135e-01,  3.4664e-02,  7.0282e-01,  6.2680e-02,\n",
      "          9.2064e-02,  2.3083e-02, -7.4810e-01, -1.2115e-01,  6.4307e-02,\n",
      "         -9.3896e-02, -6.1913e-01, -9.8255e-01, -2.6808e-02,  8.6035e-02,\n",
      "         -3.5864e-02, -5.2220e-01,  4.4491e-02, -9.9727e-01,  3.7814e-02,\n",
      "          5.8837e-01,  8.0624e-01, -3.2782e-02,  7.0775e-02,  6.7566e-01,\n",
      "         -2.4852e-01, -7.5064e-02,  1.4718e-01,  1.3661e-01, -5.7366e-03,\n",
      "         -6.6595e-01,  2.6194e-02, -6.4425e-02, -2.2900e-02,  8.2628e-02,\n",
      "         -7.4529e-02,  2.9946e-02,  8.0596e-01, -2.9831e-01,  5.9154e-02,\n",
      "          1.5634e-01,  5.2768e-01, -1.8252e-02, -2.4786e-01, -3.0855e-03,\n",
      "         -5.1980e-01,  1.3960e-02,  9.9839e-01, -6.9169e-02,  6.5970e-01,\n",
      "          7.0796e-01,  2.5834e-02,  7.3379e-02,  8.2807e-01, -6.0534e-01,\n",
      "          1.1461e-02,  5.0952e-02, -5.4335e-01,  4.2929e-02,  3.3863e-01,\n",
      "         -4.4814e-01,  5.4980e-01, -9.9869e-01,  6.0174e-02,  7.5916e-01,\n",
      "          4.5990e-01, -4.4773e-01,  7.6806e-01, -6.2420e-01, -5.9310e-01,\n",
      "         -2.2367e-02,  7.7203e-02,  2.4005e-01, -1.1879e-01, -3.2245e-02,\n",
      "          9.9922e-01,  2.4591e-02, -1.6494e-02,  3.8437e-02, -3.0519e-02,\n",
      "          1.6185e-02,  5.4753e-03, -4.0001e-03,  8.4667e-01,  6.3272e-02,\n",
      "          7.5844e-02, -7.2822e-03, -9.9433e-01, -4.9584e-01,  6.1511e-01,\n",
      "          3.0698e-02, -6.9249e-02, -7.3748e-01, -1.2543e-03, -2.7599e-02,\n",
      "          4.5774e-02, -3.5128e-01, -2.7941e-02, -1.0018e-01,  6.0464e-01,\n",
      "          2.3033e-02, -5.4916e-02, -2.6544e-02,  4.3042e-02,  1.5380e-02,\n",
      "          2.1063e-01, -6.3400e-01, -1.0751e-01,  5.6899e-01,  3.4172e-01,\n",
      "          2.6618e-02,  2.6583e-02, -3.3392e-01, -8.2124e-01, -4.9886e-01,\n",
      "          8.8936e-01,  7.2106e-02, -2.9766e-01,  1.3484e-02, -1.6160e-01,\n",
      "          8.5812e-01,  8.3372e-01, -4.3131e-01, -5.3398e-02, -9.7566e-01,\n",
      "          1.0104e-01, -8.1906e-02,  5.2914e-02]], device='cuda:0',\n",
      "       grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "# github example\n",
    "input_ids = torch.LongTensor([[31, 51, 99]]).to(device)\n",
    "input_mask = torch.LongTensor([[1, 1, 1]]).to(device)\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1]]).to(device)\n",
    "\n",
    "all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask) \n",
    "# inference 시 : token tensor, segement tensor\n",
    "# training 시 : token tensor, \n",
    "\n",
    "print(all_encoder_layers[-1][0])\n",
    "print(pooled_output.shape)\n",
    "print(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2461,  0.2428,  0.2590,  ..., -0.4861, -0.0731,  0.0756],\n",
      "        [-0.2478,  0.2420,  0.2552,  ..., -0.4877, -0.0727,  0.0754],\n",
      "        [-0.2472,  0.2420,  0.2561,  ..., -0.4874, -0.0733,  0.0765]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 768])\n",
      "tensor([[-9.0314e-02, -4.4431e-02,  1.5792e-01,  3.1426e-02, -7.4218e-01,\n",
      "          3.7952e-01,  8.3690e-02, -8.2061e-02,  5.9057e-02, -7.1027e-02,\n",
      "         -3.6518e-01, -1.0447e-01, -9.4905e-02,  1.4371e-01, -1.0304e-02,\n",
      "          4.4356e-01, -5.9710e-01,  2.8139e-03,  8.4893e-02,  8.0748e-02,\n",
      "         -1.3730e-01, -2.8074e-02,  8.6585e-02, -1.0943e-01,  1.1957e-04,\n",
      "          4.2622e-01, -7.0176e-01, -1.3383e-01, -8.7509e-01, -9.9335e-02,\n",
      "          8.5697e-01,  5.5653e-01, -1.6640e-02, -1.1087e-01,  2.0848e-01,\n",
      "          5.8470e-01, -1.1724e-02, -7.3793e-02, -4.2836e-01, -6.7545e-02,\n",
      "         -7.8296e-02, -6.5179e-02, -5.5545e-02,  6.5026e-01, -9.8818e-01,\n",
      "          6.5573e-02, -1.5565e-01, -4.3171e-02, -9.8807e-01,  9.4174e-01,\n",
      "          2.3175e-01, -9.9838e-01, -9.5131e-01,  3.1647e-02, -2.5830e-03,\n",
      "         -6.7435e-01,  1.0332e-01, -6.3440e-01,  6.6300e-02, -2.8817e-02,\n",
      "          5.3691e-03, -1.8036e-01,  8.6036e-01, -7.1559e-01, -5.2901e-02,\n",
      "          7.6154e-02,  9.3047e-02, -3.7840e-03,  1.0834e-01,  1.6944e-02,\n",
      "          8.8734e-01, -1.9038e-02, -2.2431e-01, -4.7912e-02, -4.6557e-01,\n",
      "          7.6835e-02, -3.1749e-01,  7.6349e-02,  3.3167e-01,  1.1159e-01,\n",
      "         -2.8926e-02, -4.1216e-01, -3.8193e-01,  4.4262e-01, -9.3865e-02,\n",
      "         -1.1985e-02, -1.0765e-01, -1.4020e-02,  3.2103e-01,  7.8825e-01,\n",
      "         -1.9590e-01,  3.5568e-02, -4.7788e-02, -9.3513e-02, -3.3240e-01,\n",
      "         -3.6088e-03,  8.0596e-02,  1.9113e-02,  9.5772e-02, -5.8218e-01,\n",
      "          7.1050e-02,  1.1238e-02,  9.9498e-02, -5.0844e-02, -6.1773e-01,\n",
      "          1.2965e-02, -1.3467e-01, -8.2426e-02,  1.5885e-01,  1.7332e-01,\n",
      "         -6.1233e-02,  8.2601e-02, -2.5352e-01,  2.0043e-02, -4.9155e-01,\n",
      "          4.8146e-02,  2.2087e-03,  1.7180e-02,  9.2277e-02, -3.3122e-02,\n",
      "          2.3940e-02, -5.7843e-02, -6.6485e-01,  8.3922e-01, -9.9894e-01,\n",
      "         -3.8994e-02, -1.1066e-01, -6.7163e-03,  6.5162e-02,  9.8453e-03,\n",
      "         -5.7394e-02, -2.6451e-02, -3.8337e-02, -4.6768e-02,  1.2441e-02,\n",
      "          1.6111e-01,  6.2509e-01, -6.9837e-02,  1.0481e-01,  5.3441e-02,\n",
      "         -6.1491e-02,  1.5976e-02,  3.3276e-01, -7.3973e-01, -5.3669e-02,\n",
      "         -1.5295e-02, -4.2157e-02,  5.5983e-02, -9.0542e-02,  4.1725e-02,\n",
      "         -7.5838e-01, -4.1056e-02, -3.1149e-01,  4.5713e-02,  1.4176e-02,\n",
      "         -5.4661e-02, -6.8062e-02,  2.6319e-02, -8.6462e-02, -7.5507e-01,\n",
      "          4.3413e-02, -1.3654e-02, -2.3135e-02,  9.1704e-01,  3.2953e-02,\n",
      "          3.3777e-02, -6.5220e-02, -6.5350e-02,  8.0596e-02,  3.2013e-02,\n",
      "          1.2159e-01,  3.7620e-02, -8.5716e-01,  5.1438e-01,  6.4800e-03,\n",
      "          9.9809e-01, -8.4303e-02, -2.4880e-01, -9.2737e-03, -2.3128e-02,\n",
      "         -7.5584e-02,  7.5837e-01,  2.2938e-01, -9.9882e-01, -1.7415e-02,\n",
      "         -1.3812e-02,  1.0398e-01,  6.7775e-02, -5.0627e-02, -4.3661e-01,\n",
      "         -1.1267e-01, -5.1147e-04, -5.3875e-01, -5.7024e-02, -5.0544e-02,\n",
      "          1.2913e-02,  2.0323e-01, -7.2005e-02, -1.6688e-01,  1.9091e-02,\n",
      "          5.1721e-03,  6.1205e-01,  3.7344e-02, -3.1527e-02,  8.7065e-01,\n",
      "         -5.9752e-02, -6.4937e-02,  2.6950e-02, -1.5833e-01,  3.1401e-01,\n",
      "          1.2504e-02,  1.2054e-01,  8.6126e-01,  8.2400e-01, -6.1186e-02,\n",
      "          2.9007e-03, -3.5148e-02,  2.6882e-01,  8.2610e-01, -1.1824e-01,\n",
      "         -3.6852e-01, -7.9212e-02,  6.4023e-02, -2.7358e-02,  1.0596e-01,\n",
      "         -1.5724e-01,  7.6896e-01, -7.7541e-02, -8.4581e-02,  7.1178e-02,\n",
      "         -9.2642e-01,  1.6214e-01,  1.3090e-01,  7.3327e-02,  2.4008e-01,\n",
      "         -3.4433e-01,  8.1896e-01,  2.2810e-03,  9.2115e-03, -2.6237e-02,\n",
      "          3.9763e-01, -6.6031e-02,  8.3374e-01, -1.6444e-02,  8.5033e-01,\n",
      "         -2.3064e-03,  2.0880e-02,  6.6120e-01,  1.1458e-01,  6.3873e-01,\n",
      "         -2.4924e-02, -1.9475e-02,  5.0935e-02,  3.8568e-02, -5.1232e-02,\n",
      "         -1.2023e-01, -2.1858e-01,  1.0702e-01,  7.5702e-01, -4.6442e-01,\n",
      "         -5.5578e-02, -6.2425e-02, -1.5191e-02,  9.0927e-02,  2.9525e-01,\n",
      "          6.1124e-02,  1.4597e-03, -7.1453e-01, -5.0086e-01,  6.0738e-02,\n",
      "          3.1266e-02, -4.1778e-02,  4.1208e-02,  6.2514e-01,  5.9181e-01,\n",
      "         -8.5658e-01,  4.1398e-03, -4.2715e-02, -8.0147e-02, -5.6312e-02,\n",
      "          2.0564e-02,  7.8113e-01, -7.7442e-01, -2.1610e-02,  3.6128e-02,\n",
      "         -1.1723e-02,  2.2410e-03, -1.5298e-05, -2.0352e-02, -1.2131e-01,\n",
      "          6.6504e-01,  2.6037e-02, -3.9642e-01,  5.9964e-02, -3.8031e-02,\n",
      "          2.3147e-02, -3.5461e-01,  7.7344e-01,  1.9509e-01,  2.0505e-02,\n",
      "          3.1563e-03,  4.6691e-01,  3.2952e-02, -7.6956e-02,  9.9792e-01,\n",
      "          1.1237e-02, -7.5386e-01, -7.2844e-01, -2.8443e-02,  5.2630e-02,\n",
      "          3.5432e-01,  6.3295e-02,  5.6675e-03,  5.6776e-01,  9.9925e-01,\n",
      "          7.4896e-02,  7.5899e-02, -2.5422e-02, -1.0333e-01, -1.6573e-01,\n",
      "          1.0926e-02, -2.2113e-02,  9.6460e-02,  2.4344e-02,  3.6222e-03,\n",
      "         -5.7158e-04,  1.1271e-01,  3.2878e-02, -1.2446e-01,  7.5874e-02,\n",
      "         -4.0004e-02, -2.6355e-02, -8.4984e-01,  6.1528e-01, -4.0935e-01,\n",
      "         -4.6947e-02, -5.2872e-02,  4.0501e-02, -6.8283e-02,  8.7240e-01,\n",
      "         -9.4471e-03, -8.1216e-02,  6.9631e-01, -1.5412e-01,  4.5344e-01,\n",
      "         -9.9441e-01, -5.2850e-02, -2.6124e-01, -4.2744e-02,  2.3114e-02,\n",
      "          8.6734e-03, -7.5113e-02, -8.9416e-02, -1.6106e-02, -5.1416e-02,\n",
      "          9.2850e-02, -5.7907e-01,  9.9862e-01,  5.5906e-03, -8.8403e-02,\n",
      "         -8.1817e-02, -3.5519e-02, -7.0934e-01, -1.8339e-01, -6.8476e-02,\n",
      "          4.4195e-03, -5.7513e-01, -3.4569e-02,  6.4647e-02,  5.5330e-03,\n",
      "         -3.0698e-01,  6.5975e-01, -7.8525e-02,  4.3818e-02,  1.4380e-02,\n",
      "          2.8593e-02, -4.5615e-01,  6.5649e-01, -8.7102e-02, -2.3448e-02,\n",
      "         -9.9830e-01,  5.4821e-03,  8.9399e-01,  2.7689e-01,  5.2683e-02,\n",
      "          7.6412e-02,  2.5127e-02,  1.2044e-02,  1.1589e-01,  4.5321e-02,\n",
      "         -1.1064e-01,  9.9772e-01, -4.4698e-01, -3.8849e-01,  1.0427e-02,\n",
      "         -5.3377e-01, -7.1446e-02,  6.2411e-02,  5.8820e-02, -5.0897e-02,\n",
      "          2.0484e-02,  7.9605e-02,  1.5536e-01, -3.5991e-01,  1.9688e-01,\n",
      "          8.9524e-01, -7.6532e-01,  2.1149e-01, -3.7641e-01,  1.4326e-02,\n",
      "          3.7660e-02, -9.6332e-01, -3.6127e-02,  3.6510e-02, -1.0288e-01,\n",
      "         -5.7785e-01,  3.4572e-02, -2.6450e-02,  4.8152e-02,  2.4057e-02,\n",
      "          6.2639e-02,  7.0044e-01,  1.3003e-02,  4.3146e-01,  1.5576e-01,\n",
      "          2.4087e-02, -2.0372e-02, -6.1189e-01,  9.6252e-02,  1.5901e-01,\n",
      "          1.3572e-01,  4.4082e-02, -6.3417e-01, -6.8879e-01, -2.6707e-01,\n",
      "          1.1769e-01,  2.9819e-01, -4.9979e-01, -4.6869e-01,  3.0008e-01,\n",
      "          7.4650e-01,  6.0712e-03,  9.7434e-02,  9.9899e-01, -4.4430e-01,\n",
      "          1.0142e-01, -5.1609e-02,  3.0279e-01,  6.1115e-01,  3.0389e-02,\n",
      "         -6.4598e-01,  8.4924e-02,  5.6808e-01, -3.6678e-02,  6.4545e-02,\n",
      "          4.0331e-01,  2.6545e-02, -5.9034e-01, -1.0648e-01,  4.4836e-02,\n",
      "         -6.3619e-03, -7.9639e-02, -1.9208e-02,  5.0766e-03, -4.7481e-02,\n",
      "          8.2171e-01,  1.1884e-01, -8.5065e-01,  1.1289e-02, -4.9215e-01,\n",
      "         -1.6174e-01, -8.3348e-02, -3.2785e-02, -1.6595e-01, -1.0728e-02,\n",
      "         -6.7698e-02, -1.2142e-01, -3.7773e-03,  1.1084e-01,  1.8421e-02,\n",
      "         -9.9828e-01, -6.9275e-01,  5.1014e-01, -1.8564e-01, -2.0150e-01,\n",
      "         -2.5576e-01, -1.3861e-01,  8.9640e-02,  4.7162e-02, -9.9058e-01,\n",
      "          5.0023e-02,  2.9389e-02, -3.8238e-02, -4.5870e-02, -8.3963e-01,\n",
      "          7.3587e-02,  7.3426e-01, -5.4345e-04,  6.1135e-02, -3.8408e-01,\n",
      "         -8.5322e-01, -6.1425e-02,  4.0795e-02, -2.6134e-01,  8.6320e-02,\n",
      "          5.1381e-01,  1.3489e-01, -6.9785e-01,  9.1572e-01, -1.3339e-02,\n",
      "          3.5180e-02, -5.9987e-01, -1.1428e-01, -6.9480e-01, -4.4655e-02,\n",
      "          4.9879e-01, -4.0356e-02,  1.9094e-02,  5.0713e-02,  4.2878e-02,\n",
      "         -1.7425e-02, -6.3654e-02,  4.3639e-02, -6.9016e-01,  1.8902e-03,\n",
      "         -4.0964e-01, -1.1383e-01, -7.1528e-02,  5.2935e-02, -2.0510e-02,\n",
      "          4.0144e-01, -6.7733e-02, -9.5959e-01,  6.8566e-02,  7.7552e-01,\n",
      "          6.6642e-01, -2.3484e-02, -9.5805e-02, -2.3137e-02, -9.9553e-01,\n",
      "          3.9244e-02, -9.9939e-01,  1.6120e-01,  2.7891e-01, -2.1599e-03,\n",
      "          1.8922e-03, -1.4927e-01,  2.4562e-02, -5.4461e-01, -4.3355e-01,\n",
      "          2.2279e-02, -9.9913e-01, -8.5562e-01, -2.9425e-01,  5.5037e-02,\n",
      "          7.3747e-01, -4.3466e-02,  5.6507e-01, -4.0932e-02, -3.1574e-02,\n",
      "         -1.5121e-01,  6.7271e-02,  3.2025e-02,  3.1126e-03, -5.3729e-01,\n",
      "          3.0885e-02, -1.2666e-01, -7.6214e-02, -1.8861e-02, -3.5066e-01,\n",
      "         -6.2793e-02,  7.6859e-02, -1.2170e-01, -2.4414e-01, -2.8058e-02,\n",
      "          3.7358e-02, -3.1608e-02,  4.3959e-01, -1.8598e-02, -1.5192e-01,\n",
      "         -1.3698e-02,  1.5353e-01,  7.8235e-02, -3.9719e-02,  3.8397e-01,\n",
      "         -3.8812e-02, -1.4189e-02, -8.7511e-01,  8.4346e-02,  5.9513e-01,\n",
      "         -4.5273e-01,  6.0856e-01,  9.2615e-01, -1.1388e-01,  6.1345e-01,\n",
      "          5.4191e-02, -6.2261e-01, -5.0328e-02,  2.6401e-01,  5.3169e-03,\n",
      "         -3.8857e-02, -4.7759e-02,  9.7082e-02,  7.5794e-02, -9.9631e-01,\n",
      "          1.1467e-01,  3.1150e-01, -3.9708e-01,  1.5324e-01, -2.0588e-02,\n",
      "         -5.5906e-01,  7.4084e-02,  1.0646e-01, -4.2189e-02,  1.5251e-02,\n",
      "         -3.0960e-02, -9.3060e-02, -8.0182e-01,  1.2741e-02,  7.9758e-01,\n",
      "         -5.4955e-02,  1.6908e-02, -3.6949e-02,  6.7666e-02,  9.5799e-02,\n",
      "         -1.7086e-02,  5.2665e-03,  2.7655e-01,  7.8689e-01,  6.2741e-01,\n",
      "          7.4478e-01,  3.9976e-02, -3.6003e-01, -1.6990e-02, -6.0120e-02,\n",
      "          8.6771e-03,  7.3732e-01,  3.6682e-02,  9.9941e-01,  5.9420e-01,\n",
      "         -8.6914e-01,  3.3135e-01,  3.4664e-02,  7.0282e-01,  6.2680e-02,\n",
      "          9.2064e-02,  2.3083e-02, -7.4810e-01, -1.2115e-01,  6.4307e-02,\n",
      "         -9.3896e-02, -6.1913e-01, -9.8255e-01, -2.6808e-02,  8.6035e-02,\n",
      "         -3.5864e-02, -5.2220e-01,  4.4491e-02, -9.9727e-01,  3.7814e-02,\n",
      "          5.8837e-01,  8.0624e-01, -3.2782e-02,  7.0775e-02,  6.7566e-01,\n",
      "         -2.4852e-01, -7.5064e-02,  1.4718e-01,  1.3661e-01, -5.7366e-03,\n",
      "         -6.6595e-01,  2.6194e-02, -6.4425e-02, -2.2900e-02,  8.2628e-02,\n",
      "         -7.4529e-02,  2.9946e-02,  8.0596e-01, -2.9831e-01,  5.9154e-02,\n",
      "          1.5634e-01,  5.2768e-01, -1.8252e-02, -2.4786e-01, -3.0855e-03,\n",
      "         -5.1980e-01,  1.3960e-02,  9.9839e-01, -6.9169e-02,  6.5970e-01,\n",
      "          7.0796e-01,  2.5834e-02,  7.3379e-02,  8.2807e-01, -6.0534e-01,\n",
      "          1.1461e-02,  5.0952e-02, -5.4335e-01,  4.2929e-02,  3.3863e-01,\n",
      "         -4.4814e-01,  5.4980e-01, -9.9869e-01,  6.0174e-02,  7.5916e-01,\n",
      "          4.5990e-01, -4.4773e-01,  7.6806e-01, -6.2420e-01, -5.9310e-01,\n",
      "         -2.2367e-02,  7.7203e-02,  2.4005e-01, -1.1879e-01, -3.2245e-02,\n",
      "          9.9922e-01,  2.4591e-02, -1.6494e-02,  3.8437e-02, -3.0519e-02,\n",
      "          1.6185e-02,  5.4753e-03, -4.0001e-03,  8.4667e-01,  6.3272e-02,\n",
      "          7.5844e-02, -7.2822e-03, -9.9433e-01, -4.9584e-01,  6.1511e-01,\n",
      "          3.0698e-02, -6.9249e-02, -7.3748e-01, -1.2543e-03, -2.7599e-02,\n",
      "          4.5774e-02, -3.5128e-01, -2.7941e-02, -1.0018e-01,  6.0464e-01,\n",
      "          2.3033e-02, -5.4916e-02, -2.6544e-02,  4.3042e-02,  1.5380e-02,\n",
      "          2.1063e-01, -6.3400e-01, -1.0751e-01,  5.6899e-01,  3.4172e-01,\n",
      "          2.6618e-02,  2.6583e-02, -3.3392e-01, -8.2124e-01, -4.9886e-01,\n",
      "          8.8936e-01,  7.2106e-02, -2.9766e-01,  1.3484e-02, -1.6160e-01,\n",
      "          8.5812e-01,  8.3372e-01, -4.3131e-01, -5.3398e-02, -9.7566e-01,\n",
      "          1.0104e-01, -8.1906e-02,  5.2914e-02]], device='cuda:0',\n",
      "       grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "# github example\n",
    "input_ids = torch.LongTensor([[31, 51, 99]]).to(device)\n",
    "input_mask = torch.LongTensor([[1, 1, 1]]).to(device)\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1]]).to(device)\n",
    "\n",
    "all_encoder_layers, pooled_output = model(input_ids, token_type_ids)\n",
    "\n",
    "print(all_encoder_layers[-1][0])\n",
    "print(pooled_output.shape)\n",
    "print(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
